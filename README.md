# Polyphonic Music Generation Pipeline üéπ

[![Python](https://img.shields.io/badge/Python-3.10-blue?logo=python&logoColor=white)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-Deep%20Learning-red?logo=pytorch&logoColor=white)](https://pytorch.org/)
[![Status](https://img.shields.io/badge/Status-Completed-success)]()

> **Project Goal:** A comparative study between **LSTM (Long Short-Term Memory)** and **Transformer** architectures for symbolic music generation, addressing the *Vanishing Gradient* problem in long musical sequences.

*[Developed as a Capstone Engineering Project at UFF - 2025]*

---

## üöÄ Key Features & Results

This project benchmarks two Deep Learning approaches to generate piano music based on the **MAESTRO** and **Chopin** datasets.

| Metric | LSTM Model (Baseline) | Transformer Model (Proposed) |
| :--- | :--- | :--- |
| **Architecture** | Recurrent Neural Network (RNN) | Attention-Based Mechanism |
| **Long-Term Memory** | Fails after ~20s (loses rhythm) | **High coherence** (retains motif) |
| **Training Efficiency** | Fast convergence | Computationally intensive (High VRAM) |
| **Musicality** | Good for short phrases | **Superior structural complexity** |

---

## üõ†Ô∏è Tech Stack & Engineering Challenges

| Category | Tools / Techniques |
| :--- | :--- |
| **Deep Learning** | `PyTorch`, `TensorFlow/Keras`, `Attention Mechanisms` |
| **Audio Processing** | `pretty_midi`, `music21`, `librosa`, `FluidSynth` |
| **Infrastructure** | Google Colab Pro (T4/V100 GPUs) |
| **Optimization** | Solved **OOM (Out of Memory)** errors by implementing custom batching strategies and optimizing sequence lengths for VRAM constraints. |

---

## üìÇ Project Structure

The core logic is implemented in Jupyter Notebooks using **PyTorch**.

```text
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ transformer_maestro_v2.ipynb  <-- ‚≠ê MAIN MODEL (Best Results)
‚îÇ   ‚îú‚îÄ‚îÄ lstm_maestro_v2.ipynb         <-- Baseline Model
‚îÇ   ‚îú‚îÄ‚îÄ midi_preprocessing.ipynb      <-- Data Engineering Pipeline
‚îÇ   ‚îî‚îÄ‚îÄ (legacy_experiments)/         <-- Older V1 iterations
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ Project_Report_PT.pdf         <-- Original Thesis (Portuguese)
‚îÇ   ‚îî‚îÄ‚îÄ Presentation_PT.pdf           <-- Defense Slides
‚îÇ
‚îú‚îÄ‚îÄ generated_samples/                <-- Listen to the AI output here (.wav/.mid)
‚îî‚îÄ‚îÄ requirements.txt
---
```
## üéπ How to Run (Reproduction)
Due to the size of the MAESTRO dataset (~GBs), data is not included in this repo.

1. Clone the repo

```

git clone https://github.com/SEU_USUARIO/polyphonic-music-gen.git

```

2. Download the Dataset

‚Ä¢ Get the MAESTRO (MIDI only) dataset from Magenta TensorFlow(https://magenta.tensorflow.org/datasets/maestro).

‚Ä¢ Unzip it into a folder named `data/` in the root directory.

3. Install Dependencies

```

pip install -r requirements.txt

```

4. Train

Open `notebooks/transformer_maestro_v2.ipynb` via Jupyter Lab or Google Colab and run the cells.

---

üìú Documentation
‚Ä¢ Read the Full Thesis (PT-BR)(docs/Project_Report_PT.pdf)

‚Ä¢ (English translation coming soon)

---

**Author:** Jo√£o Pedro Murad
*Production Engineer | Data Scientist | AI Enthusiast*

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue?logo=linkedin)](www.linkedin.com/jo√£opedrosmurad)
