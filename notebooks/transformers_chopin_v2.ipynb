{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-qr5huGR6GU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfF7rD5ulGnn"
      },
      "outputs": [],
      "source": [
        "# Célula de Configuração do Monitoramento (adicionar no início)\n",
        "\n",
        "# 1. Instalar bibliotecas para monitoramento\n",
        "!pip install psutil pynvml\n",
        "\n",
        "# 2. Importar tudo o que vamos precisar\n",
        "import time\n",
        "import threading\n",
        "import psutil\n",
        "import pynvml\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 3. Definir a classe que fará o monitoramento\n",
        "class ResourceMonitor:\n",
        "    def __init__(self, interval=5):\n",
        "        self.interval = interval\n",
        "        self.data = []\n",
        "        self._stop_event = threading.Event()\n",
        "        self.thread = threading.Thread(target=self.run, daemon=True)\n",
        "\n",
        "        # Inicializa a NVML para monitoramento da GPU\n",
        "        try:\n",
        "            pynvml.nvmlInit()\n",
        "            self.gpu_count = pynvml.nvmlDeviceGetCount()\n",
        "        except pynvml.NVMLError:\n",
        "            self.gpu_count = 0\n",
        "            print(\"AVISO: Placa NVIDIA não encontrada ou driver indisponível. O monitoramento da GPU será desativado.\")\n",
        "\n",
        "    def _get_gpu_ram_usage(self):\n",
        "        if self.gpu_count == 0:\n",
        "            return 0\n",
        "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "        return info.used / (1024**3)  # Convertido para GB\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"O método que roda em segundo plano para coletar dados.\"\"\"\n",
        "        start_time = time.time()\n",
        "        while not self._stop_event.is_set():\n",
        "            timestamp = time.time() - start_time\n",
        "\n",
        "            # Coleta de dados\n",
        "            sys_ram_used = psutil.virtual_memory().used / (1024**3) # GB\n",
        "            gpu_ram_used = self._get_gpu_ram_usage() # GB\n",
        "            disk_used = psutil.disk_usage('/').used / (1024**3) # GB\n",
        "\n",
        "            self.data.append([timestamp, sys_ram_used, gpu_ram_used, disk_used])\n",
        "            time.sleep(self.interval)\n",
        "\n",
        "        if self.gpu_count > 0:\n",
        "            pynvml.nvmlShutdown()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Inicia o monitoramento.\"\"\"\n",
        "        print(\"Iniciando monitoramento de recursos...\")\n",
        "        self.thread.start()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Para o monitoramento.\"\"\"\n",
        "        self._stop_event.set()\n",
        "        self.thread.join()\n",
        "        print(\"Monitoramento de recursos finalizado.\")\n",
        "        return pd.DataFrame(self.data, columns=['Tempo (s)', 'RAM Sistema (GB)', 'RAM GPU (GB)', 'Disco (GB)'])\n",
        "\n",
        "    def plot(self):\n",
        "        \"\"\"Plota os dados coletados.\"\"\"\n",
        "        df = self.stop()\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"Nenhum dado de monitoramento foi coletado.\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(3, 1, figsize=(12, 15), sharex=True)\n",
        "        fig.suptitle('Utilização de Recursos do Sistema Durante a Execução', fontsize=16)\n",
        "\n",
        "        # Gráfico de RAM do Sistema\n",
        "        axes[0].plot(df['Tempo (s)'], df['RAM Sistema (GB)'], label='RAM do Sistema Utilizada', color='blue')\n",
        "        axes[0].set_ylabel('Uso (GB)')\n",
        "        axes[0].set_title('Uso de RAM do Sistema')\n",
        "        axes[0].grid(True)\n",
        "        axes[0].legend()\n",
        "        axes[0].fill_between(df['Tempo (s)'], df['RAM Sistema (GB)'], alpha=0.1, color='blue')\n",
        "\n",
        "        # Gráfico de RAM da GPU\n",
        "        if self.gpu_count > 0:\n",
        "            axes[1].plot(df['Tempo (s)'], df['RAM GPU (GB)'], label='RAM da GPU Utilizada', color='green')\n",
        "        else:\n",
        "            axes[1].text(0.5, 0.5, 'Monitoramento de GPU não disponível', ha='center', va='center')\n",
        "        axes[1].set_ylabel('Uso (GB)')\n",
        "        axes[1].set_title('Uso de RAM da GPU')\n",
        "        axes[1].grid(True)\n",
        "        axes[1].legend()\n",
        "        axes[1].fill_between(df['Tempo (s)'], df['RAM GPU (GB)'], alpha=0.1, color='green')\n",
        "\n",
        "        # Gráfico de Uso de Disco\n",
        "        axes[2].plot(df['Tempo (s)'], df['Disco (GB)'], label='Espaço em Disco Utilizado', color='red')\n",
        "        axes[2].set_xlabel('Tempo (segundos)')\n",
        "        axes[2].set_ylabel('Uso (GB)')\n",
        "        axes[2].set_title('Uso de Disco')\n",
        "        axes[2].grid(True)\n",
        "        axes[2].legend()\n",
        "        axes[2].fill_between(df['Tempo (s)'], df['Disco (GB)'], alpha=0.1, color='red')\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "        plt.show()\n",
        "\n",
        "# 4. Iniciar o cronômetro e o monitoramento\n",
        "# (Coloque estas 2 linhas logo antes do seu código principal começar a rodar)\n",
        "tempo_inicial = time.time()\n",
        "monitor = ResourceMonitor(interval=5) # O 'interval' é em segundos\n",
        "monitor.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqp2Gwuf6zjr"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vurdQrp26zjt"
      },
      "outputs": [],
      "source": [
        "!pip install pretty_midi\n",
        "!apt-get install -y fluidsynth\n",
        "!pip install midi2audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LtdvSBL6zjt"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython import *\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "import pretty_midi\n",
        "import torch\n",
        "import math as m\n",
        "import torch.optim as optim\n",
        "import collections\n",
        "from itertools import chain\n",
        "from torch import tensor\n",
        "from midi2audio import FluidSynth\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    device = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Aiz8lOh6zju"
      },
      "source": [
        "# Dataloading and Featurizing\n",
        "Basiclly a midi(Musical Instrument Digital Interface) note conistes of four things pitch, start time, end time, and volume. What I do first is take the Midi and convert it to a tensor containing the pitch, and volume, the duritation between the last note and the current note, and duration of the note. I changed the repersenations of timings to reduce the range of the timing vaules.\n",
        "\n",
        "Then I shifted the timings to be in 16 notes, and tokenized the data, then clamped the range of some features so I could put all the vaules in one tensor without padding. I use preprocessed data because it takes ~ an hour to process maestro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDQmZvp56zju"
      },
      "outputs": [],
      "source": [
        "volume = 50\n",
        "def untenosrize(t): return [pretty_midi.containers.Note(volume, int(note[0]), float(note[1]), float(note[2])) for note in t]\n",
        "def tenosrize(r):\n",
        "    a = pretty_midi.PrettyMIDI(r)\n",
        "    tnotes = []\n",
        "    for b in a.instruments:\n",
        "        b = b.notes\n",
        "        notes =  tensor([[c.pitch, c.start, c.get_duration()] for c in b])\n",
        "        tnotes.append(notes)\n",
        "    return tnotes[0]\n",
        "def dic(t, dics):\n",
        "    shape = t.shape\n",
        "    t = t.clone()\n",
        "    for a in range(3):\n",
        "        for b in tqdm(range(len(t[:, a]))):\n",
        "            t[b, a] = dics[a].index([t[b, a].item()])\n",
        "    t = t.type(torch.int)\n",
        "    return t.reshape(shape)\n",
        "def dic(t, dics):\n",
        "    t = t.clone()\n",
        "    for a in range(3):\n",
        "        t[:, a] = tensor([dics[a].index([t[b, a].item()]) for b in range(len(t[:, a]))])\n",
        "    return t.type(torch.int)\n",
        "def undic(t, dics):\n",
        "    shape = t.shape\n",
        "    l = []\n",
        "    for a in range(3):\n",
        "        l.append(tensor(list(map(dics[a].__getitem__, t[:, a]))))\n",
        "    l = torch.stack(l, dim=1)\n",
        "    return l.reshape(shape)\n",
        "def featurize(t):\n",
        "    index = torch.argsort(t[:, 1], dim=0)\n",
        "    t = torch.stack([t[int(a)] for a in index])\n",
        "    out = t[1:, 1] - t[:-1, 1]\n",
        "    t[1:, 1]= out\n",
        "    return t\n",
        "def unfeaturize(t):\n",
        "    t = t.clone()\n",
        "    for a in range(len(t)-1):\n",
        "        out = t[a+1, 1]+ t[a, 1]\n",
        "        t[a+1, 1]= out\n",
        "    out = t[:, 2]+t[:, 1]\n",
        "    t[:, 2] = out\n",
        "    return t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4Y2bm_g6zjv"
      },
      "outputs": [],
      "source": [
        "# Substitua o bloco que cria a lista 'data' por este:\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm # Garanta que tqdm está importado aqui\n",
        "\n",
        "path = '/content/drive/MyDrive/test-code/classical-music-midi/chopin' # Verifique se este caminho está 100% correto\n",
        "data = []\n",
        "\n",
        "# Este novo loop procura arquivos diretamente na pasta 'path'\n",
        "for filename in os.listdir(path):\n",
        "    if filename.lower().endswith(('.mid', '.midi')):\n",
        "        data.append(os.path.join(path, filename))\n",
        "\n",
        "# Adicione esta linha de verificação para ter certeza:\n",
        "print(f\"VERIFICAÇÃO: Foram encontrados {len(data)} arquivos MIDI.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDwf_Nnm6zjv"
      },
      "outputs": [],
      "source": [
        "# Função de construção\n",
        "def get_dics(directory):\n",
        "    song = []\n",
        "    for t in tqdm(directory):\n",
        "        t = featurize(tenosrize(t))\n",
        "        t[:, 1] = torch.clamp(t[:, 1], max=3.9687)\n",
        "        t[:, 2] = torch.clamp(t[:, 2], min=1/time_step , max=4)\n",
        "        song.append(t)\n",
        "    t = torch.cat(song)\n",
        "    t[:, [1,2]] = torch.round(t[:, [1,2]]*time_step)/time_step\n",
        "    dics = [list(np.array(torch.unique(t[:, a]).type(torch.float))) for a in range(3)]\n",
        "    dics[0] = [np.float32(a) for a in range(128)]\n",
        "    return t, dics\n",
        "\n",
        "# Rodar tudo do zero (sem usar arquivos salvos)\n",
        "\n",
        "time_step = 32\n",
        "t, dics = get_dics(data)  # <- 'data' deve estar carregada\n",
        "\n",
        "songs = dic(t, dics)      # <- essa função também precisa estar definida corretamente\n",
        "\n",
        "# Função de reconstrução\n",
        "def unlatent(t, dics=dics):\n",
        "    t = undic(t, dics)\n",
        "    t = untenosrize(unfeaturize(t))\n",
        "    return t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMNuzts36zjw"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "sequence_len = 128\n",
        "split = torch.split(songs, sequence_len)[:-1]\n",
        "x , y = torch.stack(split)[1:], torch.stack(split)[:-1]\n",
        "dslen = len(x)//10\n",
        "xtrain, ytrain = x[:dslen*9], y[:dslen*9]\n",
        "xtest, ytest = x[dslen*9:], y[dslen*9:]\n",
        "class trainset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.x, self.y = data\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, index):\n",
        "        x = self.x[index] # Input tokens\n",
        "        y_tokens = self.y[index].long() # Target tokens, shape (seq_len, 3)\n",
        "\n",
        "        # Codifica cada atributo com seu vocabulário correto\n",
        "        y_pitch = nn.functional.one_hot(y_tokens[:, 0], num_classes=len(dics[0]))\n",
        "        y_dtime = nn.functional.one_hot(y_tokens[:, 1], num_classes=len(dics[1]))\n",
        "        y_dur = nn.functional.one_hot(y_tokens[:, 2], num_classes=len(dics[2]))\n",
        "\n",
        "        # Retorna o input e uma tupla com as 3 etiquetas codificadas\n",
        "        return x, (y_pitch, y_dtime, y_dur)\n",
        "\n",
        "train, test = trainset([xtrain, ytrain]), trainset([xtest, ytest])\n",
        "train, test = DataLoader(train, batch_size=batch_size, shuffle = True), DataLoader(test, batch_size=batch_size, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws6RQK2k6zjw"
      },
      "source": [
        "This is a test song after going through featurizing and compression, definatble noticable, but it will make the model work a lot better. Particular notes that are split into three parts aren't nicely presevered by 16th note compression. The only way i could get to display audio was through synthensizer library so that's why all the display audio sounds like it's form a syntehsizer, the higher quality unsynthesied audio is in the output of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ynZtPla6zjw"
      },
      "outputs": [],
      "source": [
        "t, tok = get_dics(data[:2])\n",
        "lat = dic(t, tok)\n",
        "qwe = unlatent(lat, tok)\n",
        "mid = pretty_midi.PrettyMIDI(data[0])\n",
        "p1 = \"test_uncompressed.mid\"\n",
        "p2 = \"test_compressed.mid\"\n",
        "mid.write(p1)\n",
        "mid.instruments[0].notes = qwe\n",
        "mid.write(p2)\n",
        "fs = 44000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2ivDT-k6zjx"
      },
      "outputs": [],
      "source": [
        "# original\n",
        "mid = pretty_midi.PrettyMIDI(p1)\n",
        "IPython.display.Audio(mid.synthesize(fs=fs), rate=fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxIgJ4Jy6zjx"
      },
      "outputs": [],
      "source": [
        "#compressed\n",
        "mid = pretty_midi.PrettyMIDI(p2)\n",
        "IPython.display.Audio(mid.synthesize(fs=fs), rate=fs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reRgATs96zjx"
      },
      "source": [
        "# Generate music with model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DPCtucQ6zjy"
      },
      "outputs": [],
      "source": [
        "p = data[0]\n",
        "numiter = range(999).__iter__()\n",
        "def gener(gen, x, dis= None, sequences=100, escape_count=10):\n",
        "    output = []\n",
        "    gen.to(device)\n",
        "    output = []\n",
        "    for a in range(sequences):\n",
        "        if dis==None:\n",
        "            x = gen(x, generate=True)\n",
        "        else:\n",
        "            dis.to(device)\n",
        "            samples = [gen(x.type(torch.int), generate=True).type(torch.float32) for a in range(escape_count)]\n",
        "            score = [dis(samples[a]) for a in range(escape_count)]\n",
        "            x = samples[score.index(max(score))]\n",
        "        output.append(x)\n",
        "    return undic(torch.cat(output, dim=1).squeeze().type(torch.int), dics)\n",
        "def make_song(gen, p=p, sequence_len=sequence_len, dis=None, sequences= 25, escape_count=10, evalu=False):\n",
        "    with torch.no_grad():\n",
        "        if evalu:\n",
        "            model.eval()\n",
        "        else: model.train()\n",
        "        x,y = next(iter(train))\n",
        "        x = x[0].unsqueeze(dim=0)\n",
        "        preds = gener(gen, x.to(device), dis=dis, sequences=sequences, escape_count=escape_count)\n",
        "        out = untenosrize(unfeaturize(preds.squeeze()))\n",
        "        mid = pretty_midi.PrettyMIDI(p)\n",
        "        mid.instruments[0].notes = out\n",
        "        itera = str(numiter.__next__())\n",
        "        mid.write(\"song \" + itera + '.mid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3gEgnjY6zjy"
      },
      "source": [
        "# Model\n",
        "I addatied this transformer from the pytorch docs (https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
        "A lot of the stuff I read said sparse transformers worked better which I might try a some point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD_XUfay6zjy"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = sequence_len):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(batch_size, max_len, 1, d_model)\n",
        "        pe[:, :, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, :, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "def compose(f, x): return f(x)\n",
        "class transformer(nn.Module):\n",
        "    def __init__(self, d_model=1024, nhead=16, d_hid=16, nlayers=6, dropout= 0.25, nembeds=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        self.embeds = nn.Embedding(nembeds, d_model)\n",
        "        self.pos_embeds = PositionalEncoding(d_model, dropout)\n",
        "        layers = nn.TransformerEncoderLayer(d_model*3, nhead, d_hid, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(layers, nlayers)\n",
        "        self.decoder = nn.Linear(d_model*3, nembeds*3)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.d_model = d_model\n",
        "        self.mask = torch.triu(torch.ones(sequence_len, sequence_len) * float('-inf'), diagonal=1).to(device)\n",
        "\n",
        "    def forward(self, x, generate=False):\n",
        "        x = self.embeds(x)* math.sqrt(self.d_model)\n",
        "        x = self.pos_embeds(x)\n",
        "        sp = x.shape\n",
        "        x = torch.reshape(x, (sp[0], sp[1], (self.d_model*3)))\n",
        "        x = self.transformer_encoder(x, self.mask)\n",
        "        x= self.decoder(x)\n",
        "        shap = x.shape\n",
        "        x = torch.reshape(x, (shap[0], shap[1], 3, shap[2]//3))\n",
        "        if generate:\n",
        "            x = torch.argmax(x, dim=3)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11GexPar6zjy"
      },
      "source": [
        "# Evaulation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrB-zYxf6zjy"
      },
      "outputs": [],
      "source": [
        "# CÓDIGO NOVO - SUBSTITUA SUA FUNÇÃO 'eval' INTEIRA POR ESTA\n",
        "\n",
        "def eval(model, dis=None):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    with torch.no_grad():\n",
        "        loss, false_preds, true_preds = [], [], []\n",
        "        count = 0\n",
        "        for x_batch, y_batch in test: # Renomeei as variáveis\n",
        "\n",
        "            # --- CORREÇÃO APLICADA AQUI ---\n",
        "            x = x_batch.to(device)\n",
        "            y = (y_batch[0].to(device), y_batch[1].to(device), y_batch[2].to(device))\n",
        "            # -----------------------------\n",
        "\n",
        "            if count == 0:\n",
        "                shape = torch.numel(x)\n",
        "                count = 1\n",
        "            preds = model(x)\n",
        "            if dis != None:\n",
        "                gen = torch.argmax(preds, dim=3)\n",
        "                dis.to(device)\n",
        "                false_preds.append(dis(gen).mean().item())\n",
        "                true_preds.append(dis(x).mean().item())\n",
        "            loss.append(creloss(preds, y).item())\n",
        "        numelloss = round((sum(loss)/len(loss))/shape, 4)\n",
        "        avloss = round(sum(loss)/len(loss), 4)\n",
        "        if dis != None:\n",
        "            false_preds = round(sum(false_preds)/len(false_preds), 4)\n",
        "            true_preds = round(sum(true_preds)/len(true_preds), 4)\n",
        "            print(\"Eval batch loss \" + str(avloss) + \" numel loss \" + str(numelloss) + \" False \" + str(false_preds) + \" True \" + str(true_preds))\n",
        "        else:\n",
        "            print(\"Eval batch loss \" + str(avloss) + \" numel loss \" + str(numelloss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PIgsCdn6zjy"
      },
      "source": [
        "# Traing loop\n",
        "Here their's a custom Cross entropy loss becuase pytorch's inbuilt CEL doesn't work on these are dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcQlmv_T6zjy"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "# CÓDIGO NOVO\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def smax(t):\n",
        "    # Usando a função softmax nativa do PyTorch, que é mais estável\n",
        "    return F.softmax(t, dim=-1)\n",
        "\n",
        "def creloss(preds, targets):\n",
        "    # preds tem shape: (batch, seq, 3, 128)\n",
        "    # targets é uma tupla com y_pitch, y_dtime, y_dur\n",
        "    y_pitch, y_dtime, y_dur = targets\n",
        "\n",
        "    # Fatiamos as previsões do modelo para corresponder ao tamanho de cada vocabulário\n",
        "    preds_pitch = preds[:, :, 0, :y_pitch.shape[-1]]\n",
        "    preds_dtime = preds[:, :, 1, :y_dtime.shape[-1]]\n",
        "    preds_dur   = preds[:, :, 2, :y_dur.shape[-1]]\n",
        "\n",
        "    # Aplicamos softmax para obter as probabilidades\n",
        "    preds_pitch = smax(preds_pitch)\n",
        "    preds_dtime = smax(preds_dtime)\n",
        "    preds_dur   = smax(preds_dur)\n",
        "\n",
        "    # Calculamos a perda para cada atributo (adicionei 1e-8 para estabilidade numérica)\n",
        "    loss_pitch = -torch.sum(y_pitch.float() * torch.log(preds_pitch + 1e-8))\n",
        "    loss_dtime = -torch.sum(y_dtime.float() * torch.log(preds_dtime + 1e-8))\n",
        "    loss_dur   = -torch.sum(y_dur.float() * torch.log(preds_dur + 1e-8))\n",
        "\n",
        "    # Retornamos a soma das perdas\n",
        "    return loss_pitch + loss_dtime + loss_dur\n",
        "# CÓDIGO NOVO (PARA COLAR NO LUGAR DO ANTIGO)\n",
        "\n",
        "def fit(model, dl, epochs=1, lr=0.0001):\n",
        "    count = 0\n",
        "    model.to(device)\n",
        "    lossfunc = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    model.train()\n",
        "    total_loss = []\n",
        "    count = 0\n",
        "    for a in range(epochs):\n",
        "        for x_batch, y_batch in tqdm(dl): # Renomeei as variáveis para clareza\n",
        "            if count == 0:\n",
        "                shape = torch.numel(x_batch)\n",
        "                count +=1\n",
        "\n",
        "            # --- CORREÇÃO APLICADA AQUI ---\n",
        "            # Move o input 'x' para o dispositivo\n",
        "            x = x_batch.to(device)\n",
        "            # Move cada tensor DENTRO da tupla 'y' para o dispositivo\n",
        "            y = (y_batch[0].to(device), y_batch[1].to(device), y_batch[2].to(device))\n",
        "            # -----------------------------\n",
        "\n",
        "            preds = model(x)\n",
        "            loss = creloss(preds, y)\n",
        "            with torch.no_grad():\n",
        "                total_loss.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avloss = sum(total_loss)/len(total_loss)\n",
        "        print(\"training batch loss \" + str(avloss) + \" numel loss \" + str(avloss/shape))\n",
        "        eval(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TaQQrLJ6zjy"
      },
      "outputs": [],
      "source": [
        "model = transformer()\n",
        "fit(model, train, lr=0.0001, epochs=50)\n",
        "torch.save(model, \"music_transformer.pkl\")\n",
        "make_song(model)\n",
        "make_song(model, evalu=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Dados extraídos do seu log de treinamento (Transformer + Chopin, Rodada 2)\n",
        "epochs = list(range(1, 51))\n",
        "\n",
        "# --- Valores de NUMEL LOSS (Perda Média por Elemento) ---\n",
        "train_numel_loss = [\n",
        "    3.2927, 2.9649, 2.8228, 2.7348, 2.6712, 2.6172, 2.5695, 2.5265, 2.4896, 2.4570,\n",
        "    2.4269, 2.3972, 2.3711, 2.3469, 2.3249, 2.3030, 2.2805, 2.2579, 2.2364, 2.2155,\n",
        "    2.1960, 2.1767, 2.1578, 2.1390, 2.1209, 2.1040, 2.0870, 2.0700, 2.0528, 2.0355,\n",
        "    2.0182, 2.0007, 1.9828, 1.9648, 1.9468, 1.9289, 1.9113, 1.8933, 1.8752, 1.8568,\n",
        "    1.8382, 1.8192, 1.8001, 1.7809, 1.7621, 1.7431, 1.7240, 1.7049, 1.6856, 1.6662\n",
        "]\n",
        "\n",
        "eval_numel_loss = [\n",
        "    1.7244, 1.6751, 1.7006, 1.7573, 1.6933, 1.6892, 1.6747, 1.7268, 1.8042, 1.7868,\n",
        "    1.7781, 1.8144, 1.7896, 1.8747, 1.7948, 1.8404, 1.8610, 1.8879, 1.8775, 1.9045,\n",
        "    1.8799, 1.8986, 1.9363, 1.9314, 1.9433, 1.9082, 1.9112, 1.9579, 1.9203, 1.9453,\n",
        "    1.9753, 1.9712, 2.0107, 1.9644, 2.0101, 2.0168, 2.0415, 2.0842, 2.0567, 2.0855,\n",
        "    2.1184, 2.1327, 2.1257, 2.1751, 2.1685, 2.1912, 2.2239, 2.2452, 2.2453, 2.2542\n",
        "]\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# --- Criar o Gráfico ---\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# Plotar a Perda Numel de Treinamento\n",
        "plt.plot(epochs, train_numel_loss, 'bo-', label='Perda de Treinamento', markersize=4)\n",
        "\n",
        "# Plotar a Perda Numel de Avaliação\n",
        "plt.plot(epochs, eval_numel_loss, 'rs-', label='Perda de Avaliação', markersize=4)\n",
        "\n",
        "# --- Adicionar Títulos e Legendas ---\n",
        "plt.title('Curva de Perda Média por Epoch', fontsize=16)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Valor da Perda', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend(fontsize=12)\n",
        "\n",
        "# Ajustar os marcadores do eixo X para mostrar mais números\n",
        "plt.xticks(np.arange(0, 51, 5)) # Marcadores a cada 5 épocas (0, 5, 10, ...)\n",
        "\n",
        "# Exibir o gráfico\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "17-zFuEt2SwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpxA9pI_6zjy"
      },
      "outputs": [],
      "source": [
        "mid = pretty_midi.PrettyMIDI(\"song 0.mid\")\n",
        "IPython.display.Audio(mid.synthesize(fs=fs), rate=fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ica3Uuz_6zjy"
      },
      "outputs": [],
      "source": [
        "mid = pretty_midi.PrettyMIDI(\"song 1.mid\")\n",
        "IPython.display.Audio(mid.synthesize(fs=fs), rate=fs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUXCVY0k6zjy"
      },
      "source": [
        "# Discriminator\n",
        "Since the transformer use dropout when generating new songs, their is some randomness in the generated sample. To take advantage of with this a Discriminator picks the best sample out of a few generated samples. The Discriminator is an lstm followed by convulations, that's heavly normalized (50% dropout)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IulKOCcR6zjy"
      },
      "outputs": [],
      "source": [
        "class discriminator(nn.Module):\n",
        "    def __init__(self, input_dim=3, lin_dim=256, lstm_dim=256, lstm_layers=4, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.droupout = nn.Dropout(dropout)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, lin_dim // 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(lin_dim // 2, lin_dim),\n",
        "            nn.LeakyReLU(0.2))\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(lstm_dim, 8, 8, 4),\n",
        "            nn.LeakyReLU(0,1),\n",
        "            nn.Conv1d(8, 1, 8, 8),\n",
        "            nn.LeakyReLU(0,1),\n",
        "            nn.Linear(3, 1))\n",
        "        self.act= nn.Sigmoid()\n",
        "        self.lin = nn.Sequential(nn.Linear(4, 1))\n",
        "        self.lstm = nn.LSTM(input_size=lin_dim, hidden_size=lstm_dim, num_layers=lstm_layers, batch_first=True, dropout=dropout, bidirectional=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.type(torch.float)\n",
        "        x = self.mlp(x)\n",
        "        x, h = self.lstm(x)\n",
        "        x = self.conv(x.permute(0, 2, 1)).permute(0, 1, 2)\n",
        "        return self.act(x.squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjM1wIhl6zjy"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def traindis(gen, dis, epochs=1, lr=0.001, noise_scale=10):\n",
        "    dis_opt =  optim.Adam(dis.parameters(), lr=lr)\n",
        "    gen.train().to(device)\n",
        "    dis.train().to(device)\n",
        "    lossfunc = nn.BCELoss().to(device)\n",
        "    for a in range(epochs):\n",
        "        for x_batch, y_batch  in tqdm(train):\n",
        "\n",
        "\n",
        "            x = x_batch.to(device)\n",
        "\n",
        "            y_real = x.type(torch.float32)\n",
        "\n",
        "            dis_opt.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                fake = gen(x, generate=True).type(torch.float32)\n",
        "\n",
        "            fake_preds = dis(fake)\n",
        "            real_preds = dis(y_real)\n",
        "\n",
        "            fake_loss = lossfunc(fake_preds, (torch.zeros_like(fake_preds)))\n",
        "            real_loss = lossfunc(real_preds,(torch.ones_like(real_preds)))\n",
        "            dis_loss = (fake_loss + real_loss)/2\n",
        "            dis_loss.backward()\n",
        "            dis_opt.step()\n",
        "        eval(model, dis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8adInBk6zjz"
      },
      "outputs": [],
      "source": [
        "dis = discriminator()\n",
        "traindis(model, dis)\n",
        "make_song(model, dis=dis)\n",
        "mid = pretty_midi.PrettyMIDI(\"song 2.mid\")\n",
        "IPython.display.Audio(mid.synthesize(fs=fs), rate=fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhIvakh6T3uA"
      },
      "outputs": [],
      "source": [
        "from midi2audio import FluidSynth\n",
        "fs = FluidSynth('/content/drive/MyDrive/soundfonts/FluidR3_GM.sf2')  # certifique-se que o arquivo .sf2 está presente\n",
        "fs.midi_to_audio('test.mid', 'test.wav')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1ljVnqH6zjz"
      },
      "source": [
        "# Conclusion\n",
        "The music is alright, about a quater the time the music localy compareble to human made samples, but it lacks any  long term conistancy. This was a good project still with lots of ways to impove this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bENQ3CSqilWI"
      },
      "outputs": [],
      "source": [
        "# Célula de Finalização e Plotagem (adicionar no final)\n",
        "\n",
        "# 1. Parar o cronômetro e calcular o tempo total\n",
        "tempo_final = time.time()\n",
        "tempo_total_segundos = tempo_final - tempo_inicial\n",
        "\n",
        "# Formatando o tempo para horas, minutos e segundos\n",
        "horas = int(tempo_total_segundos // 3600)\n",
        "minutos = int((tempo_total_segundos % 3600) // 60)\n",
        "segundos = int(tempo_total_segundos % 60)\n",
        "\n",
        "print(\"\\n--- Relatório Final de Execução ---\")\n",
        "print(f\"Tempo de Execução Total: {horas}h {minutos}min {segundos}s\")\n",
        "\n",
        "# 2. Parar o monitor e gerar o gráfico de uso de recursos\n",
        "# A função plot() já chama o stop() e processa os dados\n",
        "monitor.plot()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}